# -*- coding: utf-8 -*-
"""Decision.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NHBore1lkc0vdu8c6_87qskRLdbQOdup
"""

import pandas as pd
import numpy as np

from sklearn import tree
from sklearn.tree import DecisionTreeClassifier
import os
import matplotlib.pyplot as plt
import graphviz

import pandas as pd
df = pd.read_excel('theWorldBank.xlsx')
df

"""Bu kod bloğunda, bir "Node" sınıfı tanımlanıyor. Node sınıfı, bir karar ağacında kullanılacak düğümleri temsil eder.

init metodu, Node sınıfının constructor'ıdır. Bu metod, Node sınıfının örneklerinin oluşturulurken çağrılır ve Node sınıfının örneklerine ait özellikleri ayarlar.

Node sınıfının özellikleri şunlardır:

feature_index: Düğümün karar verme işlemini yaparken kullanılacak özellik indeksi.
threshold: Düğümün karar verme işlemini yaparken kullanılacak eşik değeri.
left: Düğümün sol tarafına bağlı olan alt düğüm.
right: Düğümün sağ tarafına bağlı olan alt düğüm.
info_gain: Düğümün karar verme işlemini yaparken kullanılan bilgi kazancı.
value: Düğümün değeri (yalnızca yaprak düğümleri için geçerlidir).
Bu özellikler, yapılandırıcı metodun parametreleri olarak verilir ve örneklerin oluşturulduğu sırada belirlenir. Örneğin, bir Node örneği oluştururken feature_index değeri verilerek Node sınıfının örneğine feature_index özelliği atanır.
"""

class Node():
    def __init__(self, feature_index=None, threshold=None, left=None, right=None, info_gain=None, value=None):
        ''' constructor ''' 
        
        # for decision node
        self.feature_index = feature_index
        self.threshold = threshold
        self.left = left
        self.right = right
        self.info_gain = info_gain
        
        # for leaf node
        self.value = value

"""# def_init__ methodu:  
Bu kod bloğu, DecisionTreeClassifier sınıfının constructor metodunu (yapılandırıcı metodu) oluşturur. Constructor metodu, sınıf örneği oluşturulduğunda otomatik olarak çalışır ve sınıfın özelliklerini ve ilk değerlerini ayarlar.

Bu constructor metodunun özellikleri şunlardır:

root: Karar ağacının kök düğümü. İlk değeri None olarak ayarlanır.
min_samples_split: Bir düğümün bölünme işlemine izin verilebilmesi için gereken minimum örnek sayısı. İlk değeri 2 olarak ayarlanır.
max_depth: Ağacın maksimum derinliği. İlk değeri 2 olarak ayarlanır.
# def build_tree:
Bu kod bloğu, DecisionTreeClassifier sınıfının build_tree() metodunu tanımlar. Bu metod, verilen bir veri kümesinden bir karar ağacı oluşturmaya yarar.

build_tree() metodu, verilen veri kümesinin (dataset) özelliklerini (X) ve hedef değerlerini (Y) ayrıştırır. Daha sonra, veri kümesindeki örnek sayısı (num_samples) ve özellik sayısı (num_features) bulunur.

Sonrasında, veri kümesinde bölme işlemi yapılıp yapılmayacağını kontrol eder. Bölme işlemi yapılır eğer veri kümesinde gereken minimum örnek sayısından fazla örnek varsa ve ağacın o anki derinliği maksimum derinlikten küçük veya eşitse.

Eğer bölme işlemi yapılacaksa, en uygun bölme işlemini bulan get_best_split() metodu çağrılır.
# def get_best_split
Bu kod bloğu, DecisionTreeClassifier sınıfının get_best_split() metodunu tanımlar. Bu metod, verilen bir veri kümesinde en uygun bölme işlemini bulmaya yarar.

get_best_split() metodu, veri kümesindeki tüm özellikleri döndürür ve her özellik için mümkün olan eşik değerlerini (threshold) döndürür. Bu sayede, veri kümesi her özellik ve her eşik değerinde bölünür. Bölme işlemi sonucu oluşan veri kümelerine "dataset_left" ve "dataset_right" adları verilir. Bu veri kümelerinin hedef değerlerine (left_y ve right_y) ait entropi değerleri information_gain() metodu ile hesaplanır. Bu değerler, veri kümesinin hedef değerlerine ait entropi değerine göre azaltılır ve böylece bilgi kazancı (info_gain) bulunur. 
# split() ve information_gain() metodları:

split() metodu, verilen bir veri kümesini verilen bir özellik ve eşik değere göre iki veri kümesine ayırmaya yarar. Bu işlem, bir for döngüsü ile yapılır ve veri kümesinde her bir örnek için verilen özellik değeri eşik değerinden küçük veya eşitse "dataset_left" veri kümesine, büyükse "dataset_right" veri kümesine eklenir.

information_gain() metodu ise, verilen veri kümelerinin entropi değerlerini kullanarak bilgi kazancını hesaplamaya yarar. Bu işlem, verilen mode değerine göre gini endeksi veya entropi hesaplaması ile yapılır. Önce veri kümelerinin hedef değerlerine ait entropi değerleri hesaplanır. Daha sonra, bu değerler veri kümesinin hedef değerlerine ait entropi değerine göre azaltılarak bilgi kazancı bulunur. Bu bilgi kazancı, veri kümelerinin yeniden bölünmesine neden olacak ve bu sayede daha anlamlı bölme işlemleri gerçekleştirilecektir.

# entropy(), gini_index() ve calculate_leaf_value() metodlarını ve print_tree() metodunun son kısmını tanımlar.

entropy() metodu, verilen bir hedef değerlerine ait entropi değerini hesaplamaya yarar. Bu işlem, hedef değerlerinin benzersiz değerlerini bulunur ve her bir benzersiz hedef değeri için o hedef değerine ait örneklerin oranının logaritmik değerini alır. Bu değerler toplamı, hedef değerlerine ait entropi değerini verir.

gini_index() metodu ise, verilen hedef değerlerine ait gini endeksini hesaplamaya yarar. Bu işlem, hedef değerlerinin benzersiz değerlerini bulunur ve her bir benzersiz hedef değeri için o hedef değerine ait örneklerin oranının karesi alınır. Bu değerler toplamı, gini endeksi değerini verir.

calculate_leaf_value() metodu ise, verilen hedef değerlerine göre bir yaprak düğümü oluşturmaya yarar. Bu işlem, hedef değerlerinin listesi oluşturulur ve bu liste içerisinde en çok tekrar eden hedef değeri bulunur. Bu hedef değeri yaprak düğümünün değeri olarak atanır.

print_tree() metodunun son kısmı ise, verilen bir karar ağacını ekrana bastırmaya yarar. Bu işlem, ağacın kök düğümünden başlanarak, her bir düğümün değerine bakılır. Eğer düğüm bir yaprak düğümü ise, o düğümün değeri ekrana
print edilir. Eğer düğüm bir karar düğümü ise, o düğümün öznitelik indisi, kullanılan eşik değeri ve bilgi kazancı değerleri ekrana bastırılır. Daha sonra, o düğümün sol ve sağ alt ağaçlarına da aynı işlem uygulanır. Bu şekilde, tüm ağaç düğümleri ekrana bastırılmış olur.

 Bu kod bloğu, karar ağacının eğitilmesi ve verilen bir veri kümesine dayalı tahminlerin yapılmasını sağlar. Öncelikle veri kümesinin sütunları birleştirilir ve **build_tree()** fonksiyonu ile ağaç oluşturulur. Daha sonra, **predict()** fonksiyonu ile verilen veri kümesindeki her bir örnek için **make_prediction()** fonksiyonu kullanılarak tahminler yapılır. make_prediction() fonksiyonu, verilen bir örnek için yapılacak tahmini belirler. Eğer düğüm bir yaprak düğümü ise, o düğümün değeri döndürülür. Eğer düğüm bir karar düğümü ise, verilen örnekteki öznitelik değeri kullanılarak, eşik değerine göre sol veya sağ alt ağaca ilerlenir ve bu süreç tekrarlandıkça tahmin yapılır.
"""

class DecisionTreeClassifier():
    def __init__(self, min_samples_split=2, max_depth=2):
        ''' constructor '''
        
        # initialize the root of the tree 
        self.root = None
        
        # stopping conditions
        self.min_samples_split = min_samples_split
        self.max_depth = max_depth
 
    def build_tree(self, dataset, curr_depth=0):
        ''' recursive function to build the tree ''' 
        
        X, Y = dataset[:,:-1], dataset[:,-1]
        num_samples, num_features = np.shape(X)
        
        # split until stopping conditions are met
        if num_samples>=self.min_samples_split and curr_depth<=self.max_depth:
            # find the best split
            best_split = self.get_best_split(dataset, num_samples, num_features)
            # check if information gain is positive
            if best_split["info_gain"]>0:
                # recur left
                left_subtree = self.build_tree(best_split["dataset_left"], curr_depth+1)
                # recur right
                right_subtree = self.build_tree(best_split["dataset_right"], curr_depth+1)
                # return decision node
                return Node(best_split["feature_index"], best_split["threshold"], 
                            left_subtree, right_subtree, best_split["info_gain"])
        
        # compute leaf node
        leaf_value = self.calculate_leaf_value(Y)
        # return leaf node
        return Node(value=leaf_value)
    
    def get_best_split(self, dataset, num_samples, num_features):
        ''' function to find the best split '''
        
        # dictionary to store the best split
        best_split = {}
        max_info_gain = -float("inf")
        
        # loop over all the features
        for feature_index in range(num_features):
            feature_values = dataset[:, feature_index]
            possible_thresholds = np.unique(feature_values)
            # loop over all the feature values present in the data
            for threshold in possible_thresholds:
                # get current split
                dataset_left, dataset_right = self.split(dataset, feature_index, threshold)
                # check if childs are not null
                if len(dataset_left)>0 and len(dataset_right)>0:
                    y, left_y, right_y = dataset[:, -1], dataset_left[:, -1], dataset_right[:, -1]
                    # compute information gain
                    curr_info_gain = self.information_gain(y, left_y, right_y, "gini")
                    # update the best split if needed
                    if curr_info_gain>max_info_gain:
                        best_split["feature_index"] = feature_index
                        best_split["threshold"] = threshold
                        best_split["dataset_left"] = dataset_left
                        best_split["dataset_right"] = dataset_right
                        best_split["info_gain"] = curr_info_gain
                        max_info_gain = curr_info_gain
                        
        # return best split
        return best_split
    
    def split(self, dataset, feature_index, threshold):
        ''' function to split the data '''
        
        dataset_left = np.array([row for row in dataset if row[feature_index]<=threshold])
        dataset_right = np.array([row for row in dataset if row[feature_index]>threshold])
        return dataset_left, dataset_right
    
    def information_gain(self, parent, l_child, r_child, mode="entropy"):
        ''' function to compute information gain '''
        
        weight_l = len(l_child) / len(parent)
        weight_r = len(r_child) / len(parent)
        if mode=="gini":
            gain = self.gini_index(parent) - (weight_l*self.gini_index(l_child) + weight_r*self.gini_index(r_child))
        else:
            gain = self.entropy(parent) - (weight_l*self.entropy(l_child) + weight_r*self.entropy(r_child))
        return gain
    
    def entropy(self, y):
        ''' function to compute entropy '''
        
        class_labels = np.unique(y)
        entropy = 0
        for cls in class_labels:
            p_cls = len(y[y == cls]) / len(y)
            entropy += -p_cls * np.log2(p_cls)
        return entropy
    
    def gini_index(self, y):
        ''' function to compute gini index '''
        
        class_labels = np.unique(y)
        gini = 0
        for cls in class_labels:
            p_cls = len(y[y == cls]) / len(y)
            gini += p_cls**2
        return 1 - gini
        
    def calculate_leaf_value(self, Y):
        ''' function to compute leaf node '''
        
        Y = list(Y)
        return max(Y, key=Y.count)
    
    def print_tree(self, tree=None, indent=" "):
        ''' function to print the tree '''
        
        if not tree:
            tree = self.root

        if tree.value is not None:
            print(tree.value)

        else:
            print("X_"+str(tree.feature_index), "<=", tree.threshold, "?", tree.info_gain)
            print("%sleft:" % (indent), end="")
            self.print_tree(tree.left, indent + indent)
            print("%sright:" % (indent), end="")
            self.print_tree(tree.right, indent + indent)
    
    def fit(self, X, Y):
        ''' function to train the tree '''
        
        dataset = np.concatenate((X, Y), axis=1)
        self.root = self.build_tree(dataset)
    
    def predict(self, X):
        ''' function to predict new dataset '''
        
        preditions = [self.make_prediction(x, self.root) for x in X]
        return preditions
    
    def make_prediction(self, x, tree):
        ''' function to predict a single data point '''
        
        if tree.value!=None: return tree.value
        feature_val = x[tree.feature_index]
        if feature_val<=tree.threshold:
            return self.make_prediction(x, tree.left)
        else:
            return self.make_prediction(x, tree.right)

"""Bu kod bloğunda, verilen bir veri kümesinden feature (özellik) değerleri ve hedef değerleri ayırılır ve bu değerler train ve test olarak ikiye bölünür.

İlk olarak, verilen veri kümesinin feature değerleri (X) ve hedef değerleri (Y) numpy dizilerine atanır. Daha sonra, train_test_split fonksiyonu kullanılarak X ve Y değerleri test ve train olarak ikiye bölünür. Bu işlemde, test_size parametresi ile test kümesinin veri kümesinin ne kadarının kullanılacağı belirlenir. Örneğin, test_size=.2 ile test kümesinin veri kümesinin %20'si kullanılır. random_state parametresi ile de veri kümesinin rastgele nasıl bölüneceği belirlenir. Bölünen veri kümesinin train ve test alt kümelerine ayrılan değerler X_train, X_test, Y_train ve Y_test değişkenlerine atanır.
"""

X = df.iloc[:, :-1].values
Y = df.iloc[:, -1].values.reshape(-1,1)
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.2, random_state=41)

"""Bu kod bloğunda, bir karar ağacı modeli oluşturulur ve eğitilir. Daha sonra bu modelin ağaç yapısı ekrana yazdırılır.

İlk olarak, DecisionTreeClassifier sınıfından bir örnek oluşturulur. Bu örnekte, min_samples_split ve max_depth parametreleri verilerek modelin nasıl eğitileceği belirlenir. min_samples_split parametresi, bir düğümün bölünme işlemine izin verilebilmesi için gereken minimum örnek sayısını belirler. max_depth parametresi ise, ağacın maksimum derinliğini belirler. Bu parametreler, modelin overfitting (aşırı uyum) problemine karşı daha dayanıklı hale getirilmesine yardımcı olur.

Daha sonra, fit metodu ile model eğitilir. Bu metodun parametreleri olarak eğitim verileri (X_train) ve hedef değerleri (Y_train) verilir.

Son olarak, print_tree metodu çağrılarak modelin ağaç yapısı ekrana yazdırılır. Bu metod, modelin tahminlerini nasıl yaptığını gösterir.
"""

classifier = DecisionTreeClassifier(min_samples_split=3, max_depth=3)
classifier.fit(X_train,Y_train)
classifier.print_tree()

"""Bu kod bloğunda, önceden eğitilmiş bir karar ağacı modeli kullanılarak test verileri üzerinde tahminler yapılır ve tahminlerin doğruluğu hesaplanır.

İlk olarak, classifier adlı modelin predict metodu kullanılarak test verileri (X_test) üzerinde tahminler yapılır ve tahminler Y_pred değişkenine atanır.

Daha sonra, accuracy_score fonksiyonu kullanılarak tahminlerin doğruluğu hesaplanır. Bu fonksiyonun parametreleri olarak gerçek hedef değerler (Y_test) ve tahmin edilen hedef değerler (Y_pred) verilir. Bu fonksiyon, tahminlerin doğruluğunu yüzde cinsinden döndürür. Örneğin, tahminlerin doğruluğunun %80 olduğu anlamına gelir.
"""

Y_pred = classifier.predict(X_test) 
from sklearn.metrics import accuracy_score
accuracy_score(Y_test, Y_pred)